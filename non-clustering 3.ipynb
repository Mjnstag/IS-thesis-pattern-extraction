{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partly from https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(\"../obfuscated_data.pkl\")\n",
    "RUG.interpolate(method='linear', inplace=True)#, limit=20)\n",
    "RUG = RUG[::10]\n",
    "dfs = [RUG.filter([i]) for i in RUG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [i.dropna() for i in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(df, name):\n",
    "\n",
    "    df2 = df.copy()\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(df2)\n",
    "\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "\n",
    "    # convert an array of values into a dataset matrix\n",
    "    def create_dataset(dataset, look_back=3):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    \n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = 3\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    \n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.001, verbose=2)\n",
    "\n",
    "    # callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "    model.fit(trainX, trainY, epochs=100, verbose=2, callbacks=[early_stopping, reduce_lr], batch_size=256)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "    # calculate root mean squared error\n",
    "    rmse_train = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "    # print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    rmse_test = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "    # print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    mae_train = tf.keras.metrics.mean_absolute_error(trainY[0], trainPredict[:,0]).numpy()\n",
    "    mae_test = tf.keras.metrics.mean_absolute_error(testY[0], testPredict[:,0]).numpy()\n",
    "\n",
    "    mape_train = tf.keras.metrics.mean_absolute_percentage_error(trainY[0], trainPredict[:,0]).numpy()\n",
    "    mape_test = tf.keras.metrics.mean_absolute_percentage_error(testY[0], testPredict[:,0]).numpy()\n",
    "\n",
    "    return (name, (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 1 - flow\n",
      "Epoch 1/100\n",
      "803/803 - 9s - loss: 0.0205 - mse: 0.0205 - lr: 0.0010 - 9s/epoch - 11ms/step\n",
      "Epoch 2/100\n",
      "803/803 - 6s - loss: 0.0051 - mse: 0.0051 - lr: 0.0010 - 6s/epoch - 7ms/step\n",
      "Epoch 3/100\n",
      "803/803 - 6s - loss: 0.0043 - mse: 0.0043 - lr: 0.0010 - 6s/epoch - 8ms/step\n",
      "Epoch 4/100\n",
      "803/803 - 6s - loss: 0.0037 - mse: 0.0037 - lr: 0.0010 - 6s/epoch - 8ms/step\n",
      "Epoch 5/100\n",
      "803/803 - 6s - loss: 0.0033 - mse: 0.0033 - lr: 0.0010 - 6s/epoch - 7ms/step\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m r \u001b[39m=\u001b[39m func(df, name)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m results\u001b[39m.\u001b[39mappend(r)\n",
      "Cell \u001b[1;32mIn[98], line 42\u001b[0m, in \u001b[0;36mfunc\u001b[1;34m(df, name)\u001b[0m\n\u001b[0;32m     40\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m1\u001b[39m))\n\u001b[0;32m     41\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 42\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX, trainY, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stopping, reduce_lr], batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m)\n\u001b[0;32m     44\u001b[0m \u001b[39m# make predictions\u001b[39;00m\n\u001b[0;32m     45\u001b[0m trainPredict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(trainX)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, df in zip([i for i in RUG], dfs):\n",
    "    print(name)\n",
    "    \n",
    "    if df.isnull().values.any():\n",
    "        print(\"NaN\")\n",
    "        results.append((name, (np.nan, np.nan)))\n",
    "        print(\"-------------\")\n",
    "        continue\n",
    "\n",
    "    r = func(df, name)\n",
    "    print(\"-------------\")\n",
    "    results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in results:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Test MAE</th>\n",
       "      <th>Train MAPE</th>\n",
       "      <th>Test MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Location 1 - flow</th>\n",
       "      <td>26.171</td>\n",
       "      <td>26.418</td>\n",
       "      <td>13.970</td>\n",
       "      <td>17.016</td>\n",
       "      <td>6.925356e+07</td>\n",
       "      <td>6.263240e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 10 - flow</th>\n",
       "      <td>17.747</td>\n",
       "      <td>21.030</td>\n",
       "      <td>5.777</td>\n",
       "      <td>6.781</td>\n",
       "      <td>4.912472e+03</td>\n",
       "      <td>2.223822e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 11 - flow</th>\n",
       "      <td>5.360</td>\n",
       "      <td>2.073</td>\n",
       "      <td>1.298</td>\n",
       "      <td>0.610</td>\n",
       "      <td>5.014563e+08</td>\n",
       "      <td>5.259237e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 11 - head</th>\n",
       "      <td>25.438</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.254</td>\n",
       "      <td>5.280000e-01</td>\n",
       "      <td>4.720000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 12 - head</th>\n",
       "      <td>0.687</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4.530000e-01</td>\n",
       "      <td>4.120000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 2 - consumption</th>\n",
       "      <td>0.284</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.238</td>\n",
       "      <td>2.172000e+01</td>\n",
       "      <td>9.884582e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 3 - consumption</th>\n",
       "      <td>0.409</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.207</td>\n",
       "      <td>6.514070e+06</td>\n",
       "      <td>6.530083e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 4 - consumption</th>\n",
       "      <td>0.354</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.306</td>\n",
       "      <td>2.295000e+00</td>\n",
       "      <td>2.198144e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 5 - consumption</th>\n",
       "      <td>0.207</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.280</td>\n",
       "      <td>5.142208e+04</td>\n",
       "      <td>1.809159e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 6 - head</th>\n",
       "      <td>0.443</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.169</td>\n",
       "      <td>8.700000e-02</td>\n",
       "      <td>1.940000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 7 - head</th>\n",
       "      <td>0.376</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.184</td>\n",
       "      <td>1.050000e-01</td>\n",
       "      <td>2.690000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 8 - flow</th>\n",
       "      <td>6.483</td>\n",
       "      <td>3.513</td>\n",
       "      <td>2.630</td>\n",
       "      <td>1.871</td>\n",
       "      <td>1.448270e+05</td>\n",
       "      <td>3.558985e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location 9 - head</th>\n",
       "      <td>0.148</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.038</td>\n",
       "      <td>7.500000e-02</td>\n",
       "      <td>7.000000e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Train RMSE  Test RMSE  Train MAE  Test MAE  \\\n",
       "Location                                                               \n",
       "Location 1 - flow             26.171     26.418     13.970    17.016   \n",
       "Location 10 - flow            17.747     21.030      5.777     6.781   \n",
       "Location 11 - flow             5.360      2.073      1.298     0.610   \n",
       "Location 11 - head            25.438      0.411      0.408     0.254   \n",
       "Location 12 - head             0.687      0.578      0.298     0.273   \n",
       "Location 2 - consumption       0.284      0.406      0.098     0.238   \n",
       "Location 3 - consumption       0.409      0.472      0.136     0.207   \n",
       "Location 4 - consumption       0.354      0.688      0.079     0.306   \n",
       "Location 5 - consumption       0.207      0.405      0.078     0.280   \n",
       "Location 6 - head              0.443      0.329      0.068     0.169   \n",
       "Location 7 - head              0.376      0.518      0.070     0.184   \n",
       "Location 8 - flow              6.483      3.513      2.630     1.871   \n",
       "Location 9 - head              0.148      0.077      0.041     0.038   \n",
       "\n",
       "                            Train MAPE     Test MAPE  \n",
       "Location                                              \n",
       "Location 1 - flow         6.925356e+07  6.263240e+02  \n",
       "Location 10 - flow        4.912472e+03  2.223822e+03  \n",
       "Location 11 - flow        5.014563e+08  5.259237e+08  \n",
       "Location 11 - head        5.280000e-01  4.720000e-01  \n",
       "Location 12 - head        4.530000e-01  4.120000e-01  \n",
       "Location 2 - consumption  2.172000e+01  9.884582e+04  \n",
       "Location 3 - consumption  6.514070e+06  6.530083e+06  \n",
       "Location 4 - consumption  2.295000e+00  2.198144e+04  \n",
       "Location 5 - consumption  5.142208e+04  1.809159e+06  \n",
       "Location 6 - head         8.700000e-02  1.940000e-01  \n",
       "Location 7 - head         1.050000e-01  2.690000e-01  \n",
       "Location 8 - flow         1.448270e+05  3.558985e+05  \n",
       "Location 9 - head         7.500000e-02  7.000000e-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_results = [[i[0], i[1][0], i[1][1], i[1][2], i[1][3], i[1][4], i[1][5]] for i in results]\n",
    "\n",
    "lstm_results = pd.DataFrame(lstm_results)\n",
    "\n",
    "lstm_results.columns = [\"Location\", \"Train RMSE\", \"Test RMSE\", \"Train MAE\", \"Test MAE\", \"Train MAPE\", \"Test MAPE\"]\n",
    "lstm_results.set_index(\"Location\", inplace=True)\n",
    "lstm_results.sort_index(inplace=True)\n",
    "lstm_results = lstm_results.astype(float).round(3)\n",
    "\n",
    "display(lstm_results)\n",
    "\n",
    "# print(lstm_results.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{3}{l}{Train} & \\multicolumn{3}{l}{Test} \\\\\n",
      "{} &    RMSE &     MAE &    MAPE &    RMSE &           MAE &          MAPE \\\\\n",
      "Location                 &         &         &         &         &               &               \\\\\n",
      "\\midrule\n",
      "Location 1 - flow        &  26.171 &  26.418 &  13.970 &  17.016 &  6.925356e+07 &  6.263240e+02 \\\\\n",
      "Location 10 - flow       &  17.747 &  21.030 &   5.777 &   6.781 &  4.912472e+03 &  2.223822e+03 \\\\\n",
      "Location 11 - flow       &   5.360 &   2.073 &   1.298 &   0.610 &  5.014563e+08 &  5.259237e+08 \\\\\n",
      "Location 11 - head       &  25.438 &   0.411 &   0.408 &   0.254 &  5.280000e-01 &  4.720000e-01 \\\\\n",
      "Location 12 - head       &   0.687 &   0.578 &   0.298 &   0.273 &  4.530000e-01 &  4.120000e-01 \\\\\n",
      "Location 2 - consumption &   0.284 &   0.406 &   0.098 &   0.238 &  2.172000e+01 &  9.884582e+04 \\\\\n",
      "Location 3 - consumption &   0.409 &   0.472 &   0.136 &   0.207 &  6.514070e+06 &  6.530083e+06 \\\\\n",
      "Location 4 - consumption &   0.354 &   0.688 &   0.079 &   0.306 &  2.295000e+00 &  2.198144e+04 \\\\\n",
      "Location 5 - consumption &   0.207 &   0.405 &   0.078 &   0.280 &  5.142208e+04 &  1.809159e+06 \\\\\n",
      "Location 6 - head        &   0.443 &   0.329 &   0.068 &   0.169 &  8.700000e-02 &  1.940000e-01 \\\\\n",
      "Location 7 - head        &   0.376 &   0.518 &   0.070 &   0.184 &  1.050000e-01 &  2.690000e-01 \\\\\n",
      "Location 8 - flow        &   6.483 &   3.513 &   2.630 &   1.871 &  1.448270e+05 &  3.558985e+05 \\\\\n",
      "Location 9 - head        &   0.148 &   0.077 &   0.041 &   0.038 &  7.500000e-02 &  7.000000e-02 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_15448\\2898845625.py:3: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(lstm_results.to_latex())\n"
     ]
    }
   ],
   "source": [
    "cols = pd.MultiIndex.from_product([['Train', 'Test'], ['RMSE', 'MAE', 'MAPE']])\n",
    "lstm_results.columns = cols\n",
    "print(lstm_results.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
