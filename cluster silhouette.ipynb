{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hanoi_scenario_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\Hanoi_CMH\\\\Scenario-1', 'RUG_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\RUG_data_5years', 'RUG_raw_csv': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_csv.csv', 'RUG_timeseries': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_timeseries.pkl', 'RUG_obfuscated': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\obfuscated_data.pkl', 'RUG_no_outliers': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\obfuscated_data_rm_outlier.pkl'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_no_outliers'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(data):\n",
    "    data_copy = data.copy()\n",
    "    groups = data_copy.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    return temp[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    data_copy = data.copy()\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(data_copy.columns) * train_percentage)\n",
    "\n",
    "    train = data_copy.iloc[:, :train_size]\n",
    "    test = data_copy.iloc[:, train_size:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "    \n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaled_list_train, scaled_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    "    \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(data_copy)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmeans(pca_data, scaled_train, scaled_test, clusters=4):\n",
    "    temp_pca_data = pca_data.copy()\n",
    "    temp_scaled_train = scaled_train.copy()\n",
    "    temp_scaled_test = scaled_test.copy()\n",
    "\n",
    "    kmeans_pca = TimeSeriesKMeans(n_clusters=clusters, metric=\"dtw\",  n_jobs=-1).fit(temp_pca_data)\n",
    "    train_pca_features = kmeans_pca.labels_\n",
    "    test_pca_features = kmeans_pca.predict(temp_scaled_test)\n",
    "\n",
    "    return train_pca_features, test_pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scaled_list_train, train_lab, test_lab, column):\n",
    "    fig, ax = plt.subplots((len(set(train_lab))))\n",
    "    fig.suptitle(column)\n",
    "    for pos, label in enumerate(set(train_lab)):\n",
    "        values = scaled_list_train[(train_lab== label).nonzero()[0]]\n",
    "        for value in values:\n",
    "            ax[pos].plot(value,c=\"gray\",alpha=0.4)\n",
    "        ax[pos].plot(np.average(values,axis=0),c=\"red\")\n",
    "\n",
    "\n",
    "    for i, ax in enumerate(ax.ravel()): # 2\n",
    "        ax.set_title(\"Cluster {}\".format(i)) # 3\n",
    "\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_cluster(column, n_cluster):\n",
    "    grouped_data = create_groups(RUG[column])\n",
    "\n",
    "    scaled_list_train, scaled_list_test = scale_data(grouped_data)\n",
    "\n",
    "    pca_features = create_pca(scaled_list_train)\n",
    "\n",
    "  \n",
    "    train_lab, test_lab = create_kmeans(pca_features, scaled_list_train, scaled_list_test, n_cluster)\n",
    "\n",
    "    plot_scores(scaled_list_train, train_lab, test_lab, column)\n",
    "    # plot_scores(column, wcss, silhouette_scores, n_cluster)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, n_cluster in zip(RUG.columns, clusters):\n",
    "    print(column)\n",
    "    average_cluster(column, n_cluster)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
