{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_no_outliers'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(data):\n",
    "    # copy data to avoid changing the original data\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # group data by day\n",
    "    groups = data_copy.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    # create a list of dataframes for each day\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    # create a single dataframe with all days as columns\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    # set index to hours and minutes\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    # reduce data to every 10 minutes\n",
    "    temp = temp[::10]\n",
    "    \n",
    "    # return transformed data \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    # copy data to avoid changing the original data\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # create train and test set based on train_percentage\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(data_copy.columns) * train_percentage)\n",
    "\n",
    "    train = data_copy.iloc[:, :train_size]\n",
    "    test = data_copy.iloc[:, train_size:]\n",
    "\n",
    "    # create scaler object\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # fit and transform scaler to train data\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "    \n",
    "    # transform test data \n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaled_list_train, scaled_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    # copy data to avoid changing the original data\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    "    \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(data_copy)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmeans(pca_data, scaled_train, scaled_test, clusters=4):\n",
    "    # copy data to avoid changing the original data\n",
    "    temp_pca_data = pca_data.copy()\n",
    "    temp_scaled_train = scaled_train.copy()\n",
    "    temp_scaled_test = scaled_test.copy()\n",
    "\n",
    "    # fit kmeans to pca data\n",
    "    kmeans_pca = TimeSeriesKMeans(n_clusters=clusters, metric=\"dtw\", n_jobs=-1).fit(temp_pca_data)\n",
    "    \n",
    "    # extract and predict cluster labels\n",
    "    train_pca_features = kmeans_pca.labels_\n",
    "    test_pca_features = kmeans_pca.predict(temp_scaled_test)\n",
    "\n",
    "    return train_pca_features, test_pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scaled_list_train, train_lab, column):\n",
    "    # create figure\n",
    "    fig, ax = plt.subplots((len(set(train_lab))))\n",
    "    fig.suptitle(column)\n",
    "\n",
    "    # For each assigned cluster label, grab all columns of the complete dataframe which have that label\n",
    "    for pos, label in enumerate(set(train_lab)):\n",
    "        values = scaled_list_train[(train_lab == label).nonzero()[0]]\n",
    "\n",
    "        # plot the average cluster silhouette and the silhouette of the individual sub-timeseries.\n",
    "        for value in values:\n",
    "            ax[pos].plot(value,c=\"gray\",alpha=0.4)\n",
    "        ax[pos].plot(np.average(values,axis=0),c=\"red\")\n",
    "\n",
    "    # set subplot titles\n",
    "    for i, ax in enumerate(ax.ravel()): \n",
    "        ax.set_title(\"Cluster {}\".format(i)) \n",
    "\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_cluster(column, n_cluster):\n",
    "    '''Driver function to call all other functions in order'''\n",
    "\n",
    "    grouped_data = create_groups(RUG[column])\n",
    "\n",
    "    scaled_list_train, scaled_list_test = scale_data(grouped_data)\n",
    "\n",
    "    pca_features = create_pca(scaled_list_train)\n",
    "  \n",
    "    train_lab, test_lab = create_kmeans(pca_features, scaled_list_train, n_cluster)\n",
    "\n",
    "    plot_scores(scaled_list_train, train_lab, test_lab, column)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls the driver function for each column in the dataframe \n",
    "# in combination with the appropriate number of clusters\n",
    "for column, n_cluster in zip(RUG.columns, clusters):\n",
    "    print(column)\n",
    "    average_cluster(column, n_cluster)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
