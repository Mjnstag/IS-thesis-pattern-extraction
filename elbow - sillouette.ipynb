{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tslearn.clustering import TimeSeriesKMeans, silhouette_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_no_outliers'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(data):\n",
    "    # copy data to avoid changing the original data\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # group data by day\n",
    "    groups = data_copy.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    # create a list of dataframes for each day\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    # create a single dataframe with all days as columns\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    # set index to hours and minutes\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    # reduce data to every 10 minutes\n",
    "    temp = temp[::10]\n",
    "    # return transformed data \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    data_copy = data.copy()\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(data_copy.columns) * train_percentage)\n",
    "\n",
    "    train = data_copy.iloc[:, :train_size]\n",
    "    test = data_copy.iloc[:, train_size:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "    \n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaled_list_train, scaled_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    "    \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(data_copy)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_sillouette(data):\n",
    "    data_copy = data.copy()\n",
    "    wcss = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for i in range(1,10):\n",
    "        # print(i)\n",
    "        kmeans_pca = TimeSeriesKMeans(n_clusters=i, metric=\"dtw\", n_jobs=-1).fit(data_copy)\n",
    "        wcss.append(kmeans_pca.inertia_)\n",
    "        try:\n",
    "            silhouette_scores.append(silhouette_score(data_copy, kmeans_pca.labels_, n_jobs=-1))\n",
    "        except:\n",
    "            silhouette_scores.append(0)\n",
    "    return wcss, silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(column, wcss, silhouette_scores, n_cluster):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    x_scale = range(1,10)\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Number of clusters')\n",
    "    ax1.set_ylabel('WCSS', color=color)\n",
    "    ax1.plot(x_scale, wcss, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Silhouette score', color=color)  \n",
    "    ax2.plot(x_scale, silhouette_scores, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    plt.title(column)\n",
    "\n",
    "    plt.axvline(x = n_cluster, color = 'r', label = 'axvline - full height', linestyle=\"dashed\")\n",
    "\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow(column, n_cluster):\n",
    "    grouped_data = create_groups(RUG[column])\n",
    "\n",
    "    scaled_list_train, scaled_list_test = scale_data(grouped_data)\n",
    "\n",
    "    pca_data = create_pca(scaled_list_train)\n",
    "\n",
    "    wcss, silhouette_scores = kmeans_sillouette(pca_data)\n",
    "\n",
    "    plot_scores(column, wcss, silhouette_scores, n_cluster)\n",
    "    return (wcss, silhouette_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wcss = []\n",
    "all_silhouette_scores = []\n",
    "\n",
    "clusters = [4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wcss = []\n",
    "all_silhouette_scores = []\n",
    "\n",
    "for column, n_cluster in zip(RUG.columns, clusters):\n",
    "    print(column)\n",
    "    scores = elbow(column, n_cluster)\n",
    "    all_wcss.append(scores[0])\n",
    "    all_silhouette_scores.append(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
