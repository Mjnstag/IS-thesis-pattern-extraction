{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import os, math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans, silhouette_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hanoi_scenario_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\Hanoi_CMH\\\\Scenario-1', 'RUG_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\RUG_data_5years', 'RUG_raw_csv': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_csv.csv', 'RUG_timeseries': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_timeseries.pkl', 'RUG_obfuscated': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\obfuscated_data.pkl'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_obfuscated'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(data):\n",
    "    data_copy = data.copy()\n",
    "    groups = data_copy.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    return temp[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    data_copy = data.copy()\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(data_copy.columns) * train_percentage)\n",
    "\n",
    "    train = data_copy.iloc[:, :train_size]\n",
    "    test = data_copy.iloc[:, train_size:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "    \n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaled_list_train, scaled_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Reduce from 4 to 3 features with PCA\n",
    "    # pca = PCA(n_components=cluster_count)\n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    "    \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(data_copy)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_sillouette(data):\n",
    "    data_copy = data.copy()\n",
    "    wcss = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for i in range(1,10):\n",
    "        print(i)\n",
    "        kmeans_pca = TimeSeriesKMeans(n_clusters=i, metric=\"dtw\", n_jobs=-1).fit(data_copy)\n",
    "        wcss.append(kmeans_pca.inertia_)\n",
    "        try:\n",
    "            silhouette_scores.append(silhouette_score(data_copy, kmeans_pca.labels_, n_jobs=-1))\n",
    "        except:\n",
    "            silhouette_scores.append(0)\n",
    "    return wcss, silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(column, wcss, silhouette_scores):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(1,10), wcss, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.title(f'{column}')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(1,10), silhouette_scores, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title(f'{column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow(column):\n",
    "    grouped_data = create_groups(RUG[column])\n",
    "\n",
    "    scaled_list_train, scaled_list_test = scale_data(grouped_data)\n",
    "\n",
    "    pca_data = create_pca(scaled_list_train)\n",
    "\n",
    "    wcss, silhouette_scores = kmeans_sillouette(pca_data)\n",
    "\n",
    "    plot_scores(column, wcss, silhouette_scores)\n",
    "    return (wcss, silhouette_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wcss = []\n",
    "all_silhouette_scores = []\n",
    "\n",
    "for column in RUG.columns:\n",
    "    print(column)\n",
    "    scores = elbow(column)\n",
    "    all_wcss.append(scores[0])\n",
    "    all_silhouette_scores.append(scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
