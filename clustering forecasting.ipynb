{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1234)\n",
    "# tf.random.set_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_no_outliers'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(col_name):\n",
    "    df = RUG[col_name].copy()\n",
    "    \n",
    "    groups = df.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "    return temp[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "\n",
    "    temp = data.copy()\n",
    "\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(temp.columns) * train_percentage)\n",
    "    \n",
    "    train = temp.iloc[:, :train_size]\n",
    "    test = temp.iloc[:, train_size:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "\n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaler, scaled_list_train, scaled_list_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    temp = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    " \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(temp)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmeans(pca_data, scaled_train, scaled_test, clusters=4):\n",
    "    temp_pca_data = pca_data.copy()\n",
    "    temp_scaled_train = scaled_train.copy()\n",
    "    temp_scaled_test = scaled_test.copy()\n",
    "\n",
    "    kmeans_pca = TimeSeriesKMeans(n_clusters=clusters, metric=\"dtw\", n_jobs=-1).fit(temp_pca_data)\n",
    "    train_pca_features = kmeans_pca.labels_\n",
    "    test_pca_features = kmeans_pca.predict(temp_scaled_test)\n",
    "\n",
    "    return train_pca_features, test_pca_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different lstm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(train1, test1, scaler, look_back=3):\n",
    "\n",
    "    training, testing = train1.copy(), test1.copy()\n",
    "\n",
    "    look_back = 3\n",
    "    \n",
    "    def create_dataset(dataset, look_back=3):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.001, verbose=2)\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in tqdm(training): \n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "            \n",
    "            # reshape into X=t and Y=t+1\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "            # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "        \n",
    "        # reshape into X=t and Y=t+1\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "        # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "    rmse_train = []\n",
    "    rmse_test = []\n",
    "\n",
    "    mae_train = []\n",
    "    mae_test = []\n",
    "\n",
    "    mape_train = []\n",
    "    mape_test = []\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in training:\n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            \n",
    "            trainPredict = model.predict(trainX, verbose=0)\n",
    "            \n",
    "            trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "            trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "            \n",
    "            trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "            trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "            \n",
    "            rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "            mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "            mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        \n",
    "        trainPredict = model.predict(trainX, verbose=0)\n",
    "        \n",
    "        trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "        \n",
    "        trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "        trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "        \n",
    "        rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "        mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "        mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "\n",
    "\n",
    "    if testing.ndim > 1:\n",
    "        for test_it in testing:   \n",
    "            try:\n",
    "                \n",
    "                test_it = test_it.reshape(-1, 1) \n",
    "                # reshape into X=t and Y=t+1\n",
    "                \n",
    "                testX, testY = create_dataset(test_it, look_back)\n",
    "            # reshape input to be [samples, time steps, features]\n",
    "                \n",
    "                testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            # make predictions\n",
    "                \n",
    "                testPredict = model.predict(testX, verbose=0)\n",
    "                # invert predictions\n",
    "            \n",
    "                testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "                testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "                testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "                testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "                # calculate different evaluation metrics\n",
    "                \n",
    "                rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "                mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "                mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "            except:\n",
    "                print(\"exception occured\")\n",
    "                rmse_train.append(-1)\n",
    "                mae_train.append(-1)\n",
    "                mape_train.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            test_it = testing\n",
    "            test_it = test_it.reshape(-1, 1) \n",
    "            # reshape into X=t and Y=t+1\n",
    "            \n",
    "            testX, testY = create_dataset(test_it, look_back)\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            \n",
    "            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        # make predictions\n",
    "            \n",
    "            testPredict = model.predict(testX, verbose=0)\n",
    "            # invert predictions\n",
    "        \n",
    "            testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "            testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "            testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "            testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "            # calculate different evaluation metrics\n",
    "            \n",
    "            rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "            mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "            mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "        except:\n",
    "            print(\"exception occured\")\n",
    "            rmse_test.append(-1)\n",
    "            mae_test.append(-1)\n",
    "            mape_test.append(-1)\n",
    "\n",
    "    return (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num of clusters per column\n",
    "\n",
    "based on elbow method and silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results = []\n",
    "for location, clust_n in zip(RUG.columns, clusters):\n",
    "    print(location)\n",
    "    data = get_data(location)\n",
    "\n",
    "    scaler, scaled_list_train, scaled_list_test = scale_data(data)\n",
    "    \n",
    "    pca_features = create_pca(scaled_list_train)\n",
    "\n",
    "    train_pca_features, test_pca_features = create_kmeans(pca_features, scaled_list_train, scaled_list_test, clust_n)\n",
    "    print(Counter(train_pca_features), Counter(test_pca_features))\n",
    "\n",
    "    for cluster in [*Counter(train_pca_features)]:\n",
    "        cluster_train = scaled_list_train[np.where(train_pca_features == cluster)]\n",
    "        cluster_test = scaled_list_test[np.where(test_pca_features == cluster)]\n",
    "\n",
    "        reply = func(cluster_train, cluster_test, scaler)\n",
    "        complete_results.append([location, [cluster, [np.mean(reply[0]), np.mean(reply[1]), np.mean(reply[2]), np.mean(reply[3]), np.mean(reply[4]), np.mean(reply[5])]]])\n",
    "\n",
    "with open (r\"results.txt\", 'wb') as f:\n",
    "    pickle.dump(complete_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
