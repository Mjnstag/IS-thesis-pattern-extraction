{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_no_outliers'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(col_name):\n",
    "    # copy data to avoid changing the original data\n",
    "    df = RUG[col_name].copy()\n",
    "    \n",
    "    # group data by day\n",
    "    groups = df.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    # create a list of dataframes for each day\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    # create a single dataframe with all days as columns\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    # set index to hours and minutes\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    # reduce data to every 10 minutes\n",
    "    temp = temp[::10]\n",
    "    \n",
    "    # return transformed data \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    # copy data to avoid changing the original data\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # create train and test set based on train_percentage\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(data_copy.columns) * train_percentage)\n",
    "\n",
    "    train = data_copy.iloc[:, :train_size]\n",
    "    test = data_copy.iloc[:, train_size:]\n",
    "\n",
    "    # create scaler object\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # fit and transform scaler to train data\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "    \n",
    "    # transform test data \n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaler, scaled_list_train, scaled_list_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    # copy data to avoid changing the original data\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    "    \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(data_copy)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmeans(pca_data, scaled_train, scaled_test, clusters=4):\n",
    "    # copy data to avoid changing the original data\n",
    "    temp_pca_data = pca_data.copy()\n",
    "    temp_scaled_train = scaled_train.copy()\n",
    "    temp_scaled_test = scaled_test.copy()\n",
    "\n",
    "    # fit kmeans to pca data\n",
    "    kmeans_pca = TimeSeriesKMeans(n_clusters=clusters, metric=\"dtw\", n_jobs=-1).fit(temp_pca_data)\n",
    "    \n",
    "    # extract and predict cluster labels\n",
    "    train_pca_features = kmeans_pca.labels_\n",
    "    test_pca_features = kmeans_pca.predict(temp_scaled_test)\n",
    "\n",
    "    return train_pca_features, test_pca_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num of clusters per column\n",
    "\n",
    "based on elbow method and silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4]\n",
    "\n",
    "# amount of iterations to use for the average\n",
    "n_iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results = []\n",
    "for location, clust_n in zip(RUG.columns, clusters):\n",
    "    print(location)\n",
    "    indice_results = []\n",
    "    # for n_iterations times, get all training data cluster labels\n",
    "    for it in range(n_iterations):\n",
    "        print(it)\n",
    "        data = get_data(location)\n",
    "\n",
    "        scaler, scaled_list_train, scaled_list_test = scale_data(data)\n",
    "        \n",
    "        pca_features = create_pca(scaled_list_train)\n",
    "\n",
    "        train_pca_features, test_pca_features = create_kmeans(pca_features, scaled_list_train, scaled_list_test, clust_n)\n",
    "\n",
    "        indice_results.append(train_pca_features)\n",
    "    complete_results.append(indice_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_stabilty = {}\n",
    "# for each location/column, check how many indices intersect between all combinations of labels\n",
    "for column, name in zip(complete_results, RUG.columns):\n",
    "    print(name)\n",
    "    run_stability = []\n",
    "    \n",
    "    # get all combinations of runs to avoid dumplicate runs\n",
    "    run_combinations = combinations(np.arange(n_iterations), 2)\n",
    "    for run_base, run_compare in run_combinations:\n",
    "        print(f\"Run {run_base} -> Run {run_compare}\")\n",
    "\n",
    "        # count how many of each cluster label there are in each run\n",
    "        run_base_stats = dict(Counter(column[run_base]))\n",
    "        run_compare_stats = dict(Counter(column[run_compare]))\n",
    "        \n",
    "        # get all combinations of labels between the two runs\n",
    "        label_combinations = set(combinations(np.concatenate((np.unique(column[run_base]), np.unique(column[run_compare]))), 2))\n",
    "\n",
    "        highest = []\n",
    "        # for each combination of labels, check how many indices intersect\n",
    "        for label_base in np.unique(column[run_base]):\n",
    "            temp_high = 0\n",
    "            for label_compare in np.unique(column[run_compare]):\n",
    "                \n",
    "                # avoid duplicate combinations\n",
    "                if (label_base, label_compare) in label_combinations:\n",
    "\n",
    "                    # get the amount of indices that intersect\n",
    "                    res = len(np.intersect1d((column[run_base] == label_base).nonzero(), (column[run_compare] == label_compare).nonzero()))\n",
    "\n",
    "                    # calculate the percentage of indices that intersect and average it between the two runs\n",
    "                    # this is done to avoid bias towards one run due to possibility of different total sizes\n",
    "                    res2 = (res/run_base_stats[label_base]*100 + res/run_compare_stats[label_compare]*100)/2\n",
    "                    \n",
    "                    # keep track of the highest percentage since this is the most likely to be of the same cluster\n",
    "                    if res2 > temp_high:\n",
    "                        temp_high = res2\n",
    "                highest.append(temp_high)\n",
    "\n",
    "        # average the highest percentage of each combination\n",
    "        run_stability.append(np.mean(highest))\n",
    "\n",
    "    # average the stability of all combinations of runs\n",
    "    col_stabilty[name] = np.mean(run_stability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with stability scores\n",
    "pd.DataFrame(col_stabilty, index=[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
