{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partly from https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_no_outliers'])\n",
    "RUG.interpolate(method='linear', inplace=True)\n",
    "RUG = RUG[::10]\n",
    "dfs = [RUG.filter([i]) for i in RUG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of seperated dataframe columns and drop NaN values\n",
    "dfs = [i.dropna() for i in dfs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(df, name):\n",
    "    # copy data to avoid changing original data\n",
    "    df2 = df.copy()\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(df2)\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "\n",
    "    # convert a datapoint (time series) to vertical format and \n",
    "    # add column for previous known values using look_back\n",
    "    def create_dataset(dataset, look_back=3):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    \n",
    "    # trainsform training and test datasets into forms usable by LSTM\n",
    "    look_back = 3\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    \n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "    # create model callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.001, verbose=2)\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "    model.fit(trainX, trainY, epochs=50, verbose=2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert scale of data back to original range\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "\n",
    "    # calculate metrics\n",
    "    rmse_train = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "    rmse_test = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "\n",
    "    mae_train = tf.keras.metrics.mean_absolute_error(trainY[0], trainPredict[:,0]).numpy()\n",
    "    mae_test = tf.keras.metrics.mean_absolute_error(testY[0], testPredict[:,0]).numpy()\n",
    "\n",
    "    mape_train = tf.keras.metrics.mean_absolute_percentage_error(trainY[0], trainPredict[:,0]).numpy()\n",
    "    mape_test = tf.keras.metrics.mean_absolute_percentage_error(testY[0], testPredict[:,0]).numpy()\n",
    "\n",
    "    return (name, (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# amount of iterations to use for the average\n",
    "n_iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n_iterations):\n",
    "    # iterate over all columns and run function\n",
    "    for name, df in zip([i for i in RUG], dfs):\n",
    "        print(name)\n",
    "        \n",
    "        # incase of error with removing NaN values, skip\n",
    "        if df.isnull().values.any():\n",
    "            print(\"NaN\")\n",
    "            results.append((name, (np.nan, np.nan)))\n",
    "            print(\"-------------\")\n",
    "            continue\n",
    "        \n",
    "        # run function for baseline algorithm\n",
    "        r = func(df, name)\n",
    "        print(\"-------------\")\n",
    "        results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe from results\n",
    "lstm_results = [[i[0], i[1][0], i[1][1], i[1][2], i[1][3], i[1][4], i[1][5]] for i in results]\n",
    "lstm_results = pd.DataFrame(lstm_results)\n",
    "\n",
    "# add column names and set index\n",
    "lstm_results.columns = [\"Location\", \"Train RMSE\", \"Test RMSE\", \"Train MAE\", \"Test MAE\", \"Train MAPE\", \"Test MAPE\"]\n",
    "lstm_results.set_index(\"Location\", inplace=True)\n",
    "lstm_results.sort_index(inplace=True)\n",
    "\n",
    "# get average scores for each location\n",
    "lstm_results = lstm_results.astype(float)\n",
    "lstm_results = lstm_results.groupby('Location').mean()\n",
    "\n",
    "# create multiindex column names\n",
    "cols = pd.MultiIndex.from_product([['Train', 'Test'], ['RMSE', 'MAE', 'MAPE']])\n",
    "lstm_results.columns = cols\n",
    "\n",
    "display(lstm_results)\n",
    "\n",
    "print(lstm_results.round(3).to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
