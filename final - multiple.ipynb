{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import os, math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1234)\n",
    "# tf.random.set_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hanoi_scenario_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\Hanoi_CMH\\\\Scenario-1', 'RUG_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\RUG_data_5years', 'RUG_raw_csv': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_csv.csv', 'RUG_timeseries': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_timeseries.pkl', 'RUG_obfuscated': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\obfuscated_data.pkl'}\n"
     ]
    }
   ],
   "source": [
    "# with open(\"options.txt\", 'r') as f:\n",
    "#     options = f.readlines()\n",
    "#     options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "# print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle('obfuscated_data.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(col_name):\n",
    "    groups = RUG['Location 2 - consumption'].groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    return temp[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "\n",
    "    temp = data.copy()\n",
    "\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(temp.columns) * train_percentage)\n",
    "    \n",
    "    train = temp.iloc[:, :train_size]\n",
    "    test = temp.iloc[:, train_size:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "\n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaler, scaled_list_train, scaled_list_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    temp = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    " \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(temp)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmeans(pca_data, scaled_train, scaled_test):\n",
    "    temp_pca_data = pca_data.copy()\n",
    "    temp_scaled_train = scaled_train.copy()\n",
    "    temp_scaled_test = scaled_test.copy()\n",
    "\n",
    "    kmeans_pca = TimeSeriesKMeans(n_clusters=4, metric=\"dtw\", n_jobs=-1).fit(temp_pca_data)\n",
    "    train_pca_features = kmeans_pca.predict(temp_scaled_train)\n",
    "    test_pca_features = kmeans_pca.predict(temp_scaled_test)\n",
    "\n",
    "    return train_pca_features, test_pca_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different lstm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(train1, test1, scaler, look_back=3):\n",
    "\n",
    "    training, testing = train1.copy(), test1.copy()\n",
    "\n",
    "    look_back = 3\n",
    "    \n",
    "    def create_dataset(dataset, look_back=3):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.001, verbose=2)\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in tqdm(training): \n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "            \n",
    "            # reshape into X=t and Y=t+1\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "            # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "        \n",
    "        # reshape into X=t and Y=t+1\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "        # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "    rmse_train = []\n",
    "    rmse_test = []\n",
    "\n",
    "    mae_train = []\n",
    "    mae_test = []\n",
    "\n",
    "    mape_train = []\n",
    "    mape_test = []\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in training:\n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            \n",
    "            trainPredict = model.predict(trainX, verbose=0)\n",
    "            \n",
    "            trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "            trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "            \n",
    "            trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "            trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "            \n",
    "            rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "            mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "            mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        \n",
    "        trainPredict = model.predict(trainX, verbose=0)\n",
    "        \n",
    "        trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "        \n",
    "        trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "        trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "        \n",
    "        rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "        mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "        mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "\n",
    "\n",
    "    if testing.ndim > 1:\n",
    "        for test_it in testing:   \n",
    "            try:\n",
    "                \n",
    "                test_it = test_it.reshape(-1, 1) \n",
    "                # reshape into X=t and Y=t+1\n",
    "                \n",
    "                testX, testY = create_dataset(testing, look_back)\n",
    "            # reshape input to be [samples, time steps, features]\n",
    "                \n",
    "                testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            # make predictions\n",
    "                \n",
    "                testPredict = model.predict(testX, verbose=0)\n",
    "                # invert predictions\n",
    "            \n",
    "                testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "                testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "                testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "                testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "                # calculate different evaluation metrics\n",
    "                \n",
    "                rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "                mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "                mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "            except:\n",
    "                print(\"exception occured\")\n",
    "                rmse_train.append(-1)\n",
    "                mae_train.append(-1)\n",
    "                mape_train.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            test_it = testing\n",
    "            test_it = test_it.reshape(-1, 1) \n",
    "            # reshape into X=t and Y=t+1\n",
    "            \n",
    "            testX, testY = create_dataset(testing, look_back)\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            \n",
    "            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        # make predictions\n",
    "            \n",
    "            testPredict = model.predict(testX, verbose=0)\n",
    "            # invert predictions\n",
    "        \n",
    "            testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "            testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "            testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "            testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "            # calculate different evaluation metrics\n",
    "            \n",
    "            rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "            mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "            mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "        except:\n",
    "            print(\"exception occured\")\n",
    "            rmse_test.append(-1)\n",
    "            mae_test.append(-1)\n",
    "            mape_test.append(-1)\n",
    "\n",
    "    return (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test)\n",
    "    # return (name, (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 1 - flow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Martin\\anaconda3\\envs\\thesis_2\\lib\\site-packages\\tslearn\\utils\\utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 1702 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "c:\\Users\\Martin\\anaconda3\\envs\\thesis_2\\lib\\site-packages\\tslearn\\utils\\utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 426 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1665, 3: 36, 2: 1}) Counter({0: 404, 3: 22})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1665/1665 [06:30<00:00,  4.27it/s]\n",
      "100%|██████████| 36/36 [00:14<00:00,  2.57it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "c:\\Users\\Martin\\anaconda3\\envs\\thesis_2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Martin\\anaconda3\\envs\\thesis_2\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 2 - consumption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Martin\\anaconda3\\envs\\thesis_2\\lib\\site-packages\\tslearn\\utils\\utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 1702 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n",
      "c:\\Users\\Martin\\anaconda3\\envs\\thesis_2\\lib\\site-packages\\tslearn\\utils\\utils.py:88: UserWarning: 2-Dimensional data passed. Assuming these are 426 1-dimensional timeseries\n",
      "  warnings.warn('2-Dimensional data passed. Assuming these are '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 1495, 3: 207}) Counter({2: 254, 3: 172})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [01:02<00:00,  3.34it/s]\n",
      " 30%|███       | 449/1495 [01:41<04:42,  3.70it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "complete_results = []\n",
    "for location in RUG.columns:\n",
    "    print(location)\n",
    "    data = get_data(location)\n",
    "\n",
    "    scaler, scaled_list_train, scaled_list_test = scale_data(data)\n",
    "    \n",
    "    pca_features = create_pca(scaled_list_train)\n",
    "\n",
    "    train_pca_features, test_pca_features = create_kmeans(pca_features, scaled_list_train, scaled_list_test)\n",
    "    print(Counter(train_pca_features), Counter(test_pca_features))\n",
    "\n",
    "    for cluster in [*Counter(train_pca_features)]:\n",
    "        cluster_train = scaled_list_train[np.where(train_pca_features == cluster)]\n",
    "        cluster_test = scaled_list_test[np.where(test_pca_features == cluster)]\n",
    "\n",
    "        reply = func(cluster_train, cluster_test, scaler)\n",
    "        \n",
    "        complete_results.append([location, [cluster, [np.mean(reply[0]), np.mean(reply[1]), np.mean(reply[2]), np.mean(reply[3]), np.mean(reply[4]), np.mean(reply[5])]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Location 1 - flow',\n",
       "  [2,\n",
       "   [0.514198227345138,\n",
       "    0.3942817697018338,\n",
       "    0.26179874,\n",
       "    0.14105849,\n",
       "    36.5469,\n",
       "    17.46008]]],\n",
       " ['Location 1 - flow',\n",
       "  [0,\n",
       "   [0.5114011038269182,\n",
       "    0.3280546002785917,\n",
       "    0.23862019,\n",
       "    0.2617714,\n",
       "    41.105156,\n",
       "    219.74074]]],\n",
       " ['Location 1 - flow', [3, [nan, nan, nan, nan, nan, nan]]]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (2) does not match length of index (26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[509], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m multi_index \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mMultiIndex\u001b[39m.\u001b[39mfrom_tuples([(Location, Cluster) \u001b[39mfor\u001b[39;00m Location \u001b[39min\u001b[39;00m RUG\u001b[39m.\u001b[39mcolumns \u001b[39mfor\u001b[39;00m Cluster \u001b[39min\u001b[39;00m [\u001b[39m*\u001b[39mCounter(train_pca_features)][:\u001b[39m2\u001b[39m]],\n\u001b[0;32m      3\u001b[0m                                        names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mLocation\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mCluster\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m cols \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mMultiIndex\u001b[39m.\u001b[39mfrom_tuples([(\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmape\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m'\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m'\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m), (\u001b[39m'\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmape\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m----> 8\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(complete_results, columns\u001b[39m=\u001b[39;49mcols,index\u001b[39m=\u001b[39;49mmulti_index)\n\u001b[0;32m      9\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\core\\frame.py:790\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    781\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    782\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    783\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m         dtype,\n\u001b[0;32m    789\u001b[0m     )\n\u001b[1;32m--> 790\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    791\u001b[0m         arrays,\n\u001b[0;32m    792\u001b[0m         columns,\n\u001b[0;32m    793\u001b[0m         index,\n\u001b[0;32m    794\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    795\u001b[0m         typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    796\u001b[0m     )\n\u001b[0;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    799\u001b[0m         data,\n\u001b[0;32m    800\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    804\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    805\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[0;32m    119\u001b[0m     \u001b[39m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     arrays, refs \u001b[39m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[0;32m    121\u001b[0m     \u001b[39m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[39m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[39m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:608\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    605\u001b[0m         val \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_multiget(val, oindex\u001b[39m.\u001b[39m_values, default\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan)\n\u001b[0;32m    607\u001b[0m     val \u001b[39m=\u001b[39m sanitize_array(val, index, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 608\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(val, index)\n\u001b[0;32m    609\u001b[0m     refs\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    611\u001b[0m homogenized\u001b[39m.\u001b[39mappend(val)\n",
      "File \u001b[1;32mc:\\Users\\mjnst\\anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\core\\common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (2) does not match length of index (26)"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "multi_index = pd.MultiIndex.from_tuples([(Location, Cluster) for Location in RUG.columns for Cluster in [*Counter(train_pca_features)][:2]],\n",
    "                                       names=['Location','Cluster'])\n",
    "\n",
    "\n",
    "cols = pd.MultiIndex.from_tuples([('Train', 'rmse'), ('Train', 'mae'), ('Train', 'mape'), ('Test', 'rmse'), ('Test', 'mae'), ('Test', 'mape')])\n",
    "\n",
    "df = pd.DataFrame(complete_results, columns=cols,index=multi_index)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
