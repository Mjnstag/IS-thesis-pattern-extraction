{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import os, math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1234)\n",
    "# tf.random.set_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hanoi_scenario_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\Hanoi_CMH\\\\Scenario-1', 'RUG_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\RUG_data_5years', 'RUG_raw_csv': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_csv.csv', 'RUG_timeseries': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_timeseries.pkl', 'RUG_obfuscated': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\obfuscated_data.pkl'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(options['RUG_obfuscated'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(col_name):\n",
    "    groups = RUG[col_name].groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    return temp[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "\n",
    "    temp = data.copy()\n",
    "\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(temp.columns) * train_percentage)\n",
    "    \n",
    "    train = temp.iloc[:, :train_size]\n",
    "    test = temp.iloc[:, train_size:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "\n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaler, scaled_list_train, scaled_list_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    temp = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    " \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(temp)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmeans(pca_data, scaled_train, scaled_test, clusters=4):\n",
    "    temp_pca_data = pca_data.copy()\n",
    "    temp_scaled_train = scaled_train.copy()\n",
    "    temp_scaled_test = scaled_test.copy()\n",
    "\n",
    "    kmeans_pca = TimeSeriesKMeans(n_clusters=clusters, metric=\"dtw\", n_jobs=-1).fit(temp_pca_data)\n",
    "    train_pca_features = kmeans_pca.predict(temp_scaled_train)\n",
    "    test_pca_features = kmeans_pca.predict(temp_scaled_test)\n",
    "\n",
    "    return train_pca_features, test_pca_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different lstm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(train1, test1, scaler, look_back=3):\n",
    "\n",
    "    training, testing = train1.copy(), test1.copy()\n",
    "\n",
    "    look_back = 3\n",
    "    \n",
    "    def create_dataset(dataset, look_back=3):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.001, verbose=2)\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in tqdm(training): \n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "            \n",
    "            # reshape into X=t and Y=t+1\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "            # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "        \n",
    "        # reshape into X=t and Y=t+1\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "        # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "    rmse_train = []\n",
    "    rmse_test = []\n",
    "\n",
    "    mae_train = []\n",
    "    mae_test = []\n",
    "\n",
    "    mape_train = []\n",
    "    mape_test = []\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in training:\n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            \n",
    "            trainPredict = model.predict(trainX, verbose=0)\n",
    "            \n",
    "            trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "            trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "            \n",
    "            trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "            trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "            \n",
    "            rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "            mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "            mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        \n",
    "        trainPredict = model.predict(trainX, verbose=0)\n",
    "        \n",
    "        trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "        \n",
    "        trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "        trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "        \n",
    "        rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "        mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "        mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "\n",
    "\n",
    "    if testing.ndim > 1:\n",
    "        for test_it in testing:   \n",
    "            try:\n",
    "                \n",
    "                test_it = test_it.reshape(-1, 1) \n",
    "                # reshape into X=t and Y=t+1\n",
    "                \n",
    "                testX, testY = create_dataset(test_it, look_back)\n",
    "            # reshape input to be [samples, time steps, features]\n",
    "                \n",
    "                testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            # make predictions\n",
    "                \n",
    "                testPredict = model.predict(testX, verbose=0)\n",
    "                # invert predictions\n",
    "            \n",
    "                testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "                testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "                testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "                testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "                # calculate different evaluation metrics\n",
    "                \n",
    "                rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "                mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "                mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "            except:\n",
    "                print(\"exception occured\")\n",
    "                rmse_train.append(-1)\n",
    "                mae_train.append(-1)\n",
    "                mape_train.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            test_it = testing\n",
    "            test_it = test_it.reshape(-1, 1) \n",
    "            # reshape into X=t and Y=t+1\n",
    "            \n",
    "            testX, testY = create_dataset(test_it, look_back)\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            \n",
    "            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        # make predictions\n",
    "            \n",
    "            testPredict = model.predict(testX, verbose=0)\n",
    "            # invert predictions\n",
    "        \n",
    "            testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "            testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "            testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "            testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "            # calculate different evaluation metrics\n",
    "            \n",
    "            rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "            mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "            mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "        except:\n",
    "            print(\"exception occured\")\n",
    "            rmse_test.append(-1)\n",
    "            mae_test.append(-1)\n",
    "            mape_test.append(-1)\n",
    "\n",
    "    return (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test)\n",
    "    # return (name, (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num of clusters per column\n",
    "\n",
    "based on elbow method and silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 1 - flow\n",
      "Counter({0: 1686, 2: 18}) Counter({0: 426})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1686/1686 [18:56<00:00,  1.48it/s]\n",
      "100%|██████████| 18/18 [00:27<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 2 - consumption\n",
      "Counter({3: 1550, 1: 149, 2: 3}) Counter({3: 259, 1: 166, 2: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1550/1550 [09:41<00:00,  2.67it/s]\n",
      "100%|██████████| 149/149 [01:15<00:00,  1.98it/s]\n",
      "100%|██████████| 3/3 [00:06<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 3 - consumption\n",
      "Counter({1: 1119, 0: 261}) Counter({1: 266, 0: 79})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1119/1119 [11:16<00:00,  1.65it/s]\n",
      "100%|██████████| 261/261 [01:52<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 4 - consumption\n",
      "Counter({1: 1104, 2: 600}) Counter({1: 425, 2: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1104/1104 [08:37<00:00,  2.13it/s]\n",
      "100%|██████████| 600/600 [03:29<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 5 - consumption\n",
      "Counter({3: 1462, 0: 242}) Counter({3: 245, 0: 181})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1462/1462 [08:32<00:00,  2.85it/s]\n",
      "100%|██████████| 242/242 [01:41<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 6 - head\n",
      "Counter({1: 1704}) Counter({1: 426})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1704/1704 [08:40<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 7 - head\n",
      "Counter({3: 1538, 1: 151, 2: 15}) Counter({3: 323, 1: 103})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1538/1538 [08:32<00:00,  3.00it/s]\n",
      "100%|██████████| 151/151 [01:08<00:00,  2.19it/s]\n",
      "100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 8 - flow\n",
      "Counter({2: 1614, 1: 85, 0: 1}) Counter({2: 422, 1: 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1614/1614 [08:56<00:00,  3.01it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.10s/it]\n",
      "100%|██████████| 85/85 [00:36<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 9 - head\n",
      "Counter({0: 1214, 2: 490}) Counter({0: 426})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490/490 [02:52<00:00,  2.84it/s]\n",
      "100%|██████████| 1214/1214 [08:09<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 10 - flow\n",
      "Counter({0: 748, 1: 568, 3: 388}) Counter({0: 232, 3: 98, 1: 96})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [06:15<00:00,  1.99it/s]\n",
      "100%|██████████| 388/388 [03:53<00:00,  1.66it/s]\n",
      "100%|██████████| 568/568 [04:37<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 11 - head\n",
      "Counter({0: 1703, 2: 1}) Counter({0: 426})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1703/1703 [20:09<00:00,  1.41it/s] \n",
      "100%|██████████| 1/1 [00:03<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 11 - flow\n",
      "Counter({0: 1535, 2: 105}) Counter({0: 410})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1535/1535 [08:03<00:00,  3.18it/s]\n",
      "100%|██████████| 105/105 [01:31<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 12 - head\n",
      "Counter({1: 1703, 3: 1}) Counter({1: 426})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1703/1703 [09:13<00:00,  3.08it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "complete_results = []\n",
    "for location, clust_n in zip(RUG.columns, clusters):\n",
    "    print(location)\n",
    "    data = get_data(location)\n",
    "\n",
    "    scaler, scaled_list_train, scaled_list_test = scale_data(data)\n",
    "    \n",
    "    pca_features = create_pca(scaled_list_train)\n",
    "\n",
    "    train_pca_features, test_pca_features = create_kmeans(pca_features, scaled_list_train, scaled_list_test, clust_n)\n",
    "    print(Counter(train_pca_features), Counter(test_pca_features))\n",
    "\n",
    "    with open (fr\"C:\\Users\\mjnst\\Desktop\\results\\run_2_indices_{location}.txt\", 'wb') as f:\n",
    "        pickle.dump([train_pca_features, test_pca_features], f)\n",
    "\n",
    "    for cluster in [*Counter(train_pca_features)]:\n",
    "        cluster_train = scaled_list_train[np.where(train_pca_features == cluster)]\n",
    "        cluster_test = scaled_list_test[np.where(test_pca_features == cluster)]\n",
    "\n",
    "        reply = func(cluster_train, cluster_test, scaler)\n",
    "        # print([location, [cluster, [np.mean(reply[0]), np.mean(reply[1]), np.mean(reply[2]), np.mean(reply[3]), np.mean(reply[4]), np.mean(reply[5])]]])\n",
    "        complete_results.append([location, [cluster, [np.mean(reply[0]), np.mean(reply[1]), np.mean(reply[2]), np.mean(reply[3]), np.mean(reply[4]), np.mean(reply[5])]]])\n",
    "\n",
    "with open (r\"C:\\Users\\mjnst\\Desktop\\results\\run_2.txt\", 'wb') as f:\n",
    "    pickle.dump(complete_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
