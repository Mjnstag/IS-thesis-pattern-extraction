{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import os, math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1234)\n",
    "# tf.random.set_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hanoi_scenario_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\Hanoi_CMH\\\\Scenario-1', 'RUG_dir': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\RUG_data_5years', 'RUG_raw_csv': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_csv.csv', 'RUG_timeseries': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\rug_timeseries.pkl', 'RUG_obfuscated': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\obfuscated_data.pkl', 'RUG_no_outliers': 'C:\\\\Users\\\\mjnst\\\\Desktop\\\\Thesis\\\\obfuscated_data_rm_outlier.pkl'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"options.txt\", 'r') as f:\n",
    "    options = f.readlines()\n",
    "    options = {option.split(\"=\")[0]: option.split(\"=\")[1].strip() for option in options}\n",
    "print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG = pd.read_pickle(r'C:\\Users\\Martin\\Desktop\\thesis 2\\obfuscated_data_rm_outlier.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.interpolate(method='linear', inplace=True, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(col_name):\n",
    "    df = RUG[col_name].copy()\n",
    "    \n",
    "    groups = df.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "    # get the calender date of the groups\n",
    "    days = list(groups.first().index.strftime('%Y:%m:%d'))\n",
    "\n",
    "    gro = [groups.get_group(x).reset_index(drop=True) for x in groups.groups]\n",
    "\n",
    "    temp = pd.concat(gro, axis=1, keys=days)\n",
    "\n",
    "    temp.index = pd.date_range(\"00:00\", \"23:59\", freq=\"1min\").strftime('%H:%M')\n",
    "\n",
    "    # drop all columns of temp dataframe which contain nan values\n",
    "    temp.dropna(axis=1, how='any', inplace=True)\n",
    "    return temp[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "\n",
    "    temp = data.copy()\n",
    "\n",
    "    train_percentage = 0.8\n",
    "    train_size = int(len(temp.columns) * train_percentage)\n",
    "    \n",
    "    train = temp.iloc[:, :train_size]\n",
    "    test = temp.iloc[:, train_size:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_list_train = [train[col] for col in train]\n",
    "    scaled_list_train = scaler.fit_transform(scaled_list_train)\n",
    "\n",
    "    scaled_list_test = [test[col] for col in test]\n",
    "    scaled_list_test = scaler.transform(scaled_list_test)\n",
    "\n",
    "    return scaler, scaled_list_train, scaled_list_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(data):\n",
    "    temp = data.copy()\n",
    "    \n",
    "    pca = PCA(n_components=0.85, svd_solver='full')\n",
    " \n",
    "    # Fit and transform data\n",
    "    pca_features = pca.fit_transform(temp)\n",
    "\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmeans(pca_data, scaled_train, scaled_test, clusters=4):\n",
    "    temp_pca_data = pca_data.copy()\n",
    "    temp_scaled_train = scaled_train.copy()\n",
    "    temp_scaled_test = scaled_test.copy()\n",
    "\n",
    "    kmeans_pca = TimeSeriesKMeans(n_clusters=clusters, metric=\"dtw\", n_jobs=-1).fit(temp_pca_data)\n",
    "    train_pca_features = kmeans_pca.labels_\n",
    "    test_pca_features = kmeans_pca.predict(temp_scaled_test)\n",
    "\n",
    "    return train_pca_features, test_pca_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different lstm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(train1, test1, scaler, look_back=3):\n",
    "\n",
    "    training, testing = train1.copy(), test1.copy()\n",
    "\n",
    "    look_back = 3\n",
    "    \n",
    "    def create_dataset(dataset, look_back=3):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.001, verbose=2)\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in tqdm(training): \n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "            \n",
    "            # reshape into X=t and Y=t+1\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "            # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "        \n",
    "        # reshape into X=t and Y=t+1\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "        # testX, testY = create_dataset(testing, look_back)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        model.fit(trainX, trainY, epochs=50, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "    rmse_train = []\n",
    "    rmse_test = []\n",
    "\n",
    "    mae_train = []\n",
    "    mae_test = []\n",
    "\n",
    "    mape_train = []\n",
    "    mape_test = []\n",
    "\n",
    "    if training.ndim > 1:\n",
    "        for train_it in training:\n",
    "            train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "            trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            \n",
    "            trainPredict = model.predict(trainX, verbose=0)\n",
    "            \n",
    "            trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "            trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "            \n",
    "            trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "            trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "            \n",
    "            rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "            mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "            mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "    else:\n",
    "        train_it = training\n",
    "        train_it = train_it.reshape(-1, 1)\n",
    "\n",
    "        trainX, trainY = create_dataset(train_it, look_back)\n",
    "\n",
    "        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "        \n",
    "        trainPredict = model.predict(trainX, verbose=0)\n",
    "        \n",
    "        trainPredict = np.repeat(trainPredict, train1.shape[1], axis=-1)\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)[:,0]\n",
    "        \n",
    "        trainY = np.repeat(trainY.reshape(-1, 1), train1.shape[1], axis=-1)\n",
    "        trainY = scaler.inverse_transform(trainY)[:,0]\n",
    "        \n",
    "        rmse_train.append(np.sqrt(mean_squared_error(trainY, trainPredict)))\n",
    "        mae_train.append(tf.keras.metrics.mean_absolute_error(trainY, trainPredict).numpy())\n",
    "        mape_train.append(tf.keras.metrics.mean_absolute_percentage_error(trainY, trainPredict).numpy())\n",
    "\n",
    "\n",
    "    if testing.ndim > 1:\n",
    "        for test_it in testing:   \n",
    "            try:\n",
    "                \n",
    "                test_it = test_it.reshape(-1, 1) \n",
    "                # reshape into X=t and Y=t+1\n",
    "                \n",
    "                testX, testY = create_dataset(test_it, look_back)\n",
    "            # reshape input to be [samples, time steps, features]\n",
    "                \n",
    "                testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "            # make predictions\n",
    "                \n",
    "                testPredict = model.predict(testX, verbose=0)\n",
    "                # invert predictions\n",
    "            \n",
    "                testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "                testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "                testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "                testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "                # calculate different evaluation metrics\n",
    "                \n",
    "                rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "                mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "                mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "            except:\n",
    "                print(\"exception occured\")\n",
    "                rmse_train.append(-1)\n",
    "                mae_train.append(-1)\n",
    "                mape_train.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            test_it = testing\n",
    "            test_it = test_it.reshape(-1, 1) \n",
    "            # reshape into X=t and Y=t+1\n",
    "            \n",
    "            testX, testY = create_dataset(test_it, look_back)\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "            \n",
    "            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        # make predictions\n",
    "            \n",
    "            testPredict = model.predict(testX, verbose=0)\n",
    "            # invert predictions\n",
    "        \n",
    "            testPredict = np.repeat(testPredict, test1.shape[1], axis=-1)\n",
    "            testPredict = scaler.inverse_transform(testPredict)[:,0]\n",
    "\n",
    "            testY = np.repeat(testY.reshape(-1, 1), test1.shape[1], axis=-1)\n",
    "            testY = scaler.inverse_transform(testY)[:,0]\n",
    "\n",
    "            # calculate different evaluation metrics\n",
    "            \n",
    "            rmse_test.append(np.sqrt(mean_squared_error(testY, testPredict)))\n",
    "            mae_test.append(tf.keras.metrics.mean_absolute_error(testY, testPredict).numpy())\n",
    "            mape_test.append(tf.keras.metrics.mean_absolute_percentage_error(testY, testPredict).numpy())\n",
    "        except:\n",
    "            print(\"exception occured\")\n",
    "            rmse_test.append(-1)\n",
    "            mae_test.append(-1)\n",
    "            mape_test.append(-1)\n",
    "\n",
    "    return (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test)\n",
    "    # return (name, (rmse_train, rmse_test, mae_train, mae_test, mape_train, mape_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num of clusters per column\n",
    "\n",
    "based on elbow method and silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 1 - flow\n",
      "Counter({1: 723, 3: 417, 2: 409, 0: 155}) Counter({1: 281, 2: 143, 3: 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [01:13<00:00,  2.10it/s]\n",
      "100%|██████████| 409/409 [02:31<00:00,  2.70it/s]\n",
      "100%|██████████| 417/417 [02:05<00:00,  3.31it/s]\n",
      "100%|██████████| 723/723 [04:19<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 2 - consumption\n",
      "Counter({0: 893, 3: 580, 1: 192, 2: 37}) Counter({0: 366, 3: 56, 1: 4})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [01:06<00:00,  2.88it/s]\n",
      "100%|██████████| 893/893 [03:54<00:00,  3.82it/s]\n",
      "100%|██████████| 580/580 [02:35<00:00,  3.73it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 3 - consumption\n",
      "Counter({1: 923, 2: 447, 0: 10}) Counter({1: 287, 2: 58})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 923/923 [05:14<00:00,  2.93it/s]\n",
      "100%|██████████| 447/447 [02:15<00:00,  3.29it/s]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 4 - consumption\n",
      "Counter({1: 1129, 0: 370, 2: 205}) Counter({1: 425, 0: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1129/1129 [05:48<00:00,  3.24it/s]\n",
      "100%|██████████| 370/370 [01:29<00:00,  4.11it/s]\n",
      "100%|██████████| 205/205 [00:50<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 5 - consumption\n",
      "Counter({0: 1345, 1: 351, 2: 5, 3: 3}) Counter({0: 418, 1: 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1345/1345 [04:46<00:00,  4.69it/s]\n",
      "100%|██████████| 351/351 [01:24<00:00,  4.17it/s]\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.30s/it]\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 6 - head\n",
      "Counter({2: 1354, 0: 346, 3: 3, 1: 1}) Counter({3: 426})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1354/1354 [04:09<00:00,  5.42it/s]\n",
      "100%|██████████| 346/346 [01:15<00:00,  4.61it/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 7 - head\n",
      "Counter({0: 1096, 2: 414, 1: 101, 3: 93}) Counter({2: 425, 1: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1096/1096 [03:33<00:00,  5.14it/s]\n",
      "100%|██████████| 414/414 [01:40<00:00,  4.10it/s]\n",
      "100%|██████████| 93/93 [00:32<00:00,  2.86it/s]\n",
      "100%|██████████| 101/101 [00:35<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 8 - flow\n",
      "Counter({0: 1272, 2: 312, 1: 116}) Counter({2: 425})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1272/1272 [03:54<00:00,  5.43it/s]\n",
      "100%|██████████| 312/312 [01:09<00:00,  4.47it/s]\n",
      "100%|██████████| 116/116 [00:31<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 9 - head\n",
      "Counter({0: 1130, 1: 329, 2: 245}) Counter({0: 426})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:54<00:00,  4.48it/s]\n",
      "100%|██████████| 329/329 [01:14<00:00,  4.44it/s]\n",
      "100%|██████████| 1130/1130 [05:00<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 10 - flow\n",
      "Counter({1: 903, 0: 368, 3: 312, 2: 121}) Counter({1: 346, 3: 73, 0: 7})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 903/903 [04:14<00:00,  3.55it/s]\n",
      "100%|██████████| 368/368 [02:13<00:00,  2.76it/s]\n",
      "100%|██████████| 121/121 [00:36<00:00,  3.30it/s]\n",
      "100%|██████████| 312/312 [01:28<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 11 - head\n",
      "Counter({0: 965, 2: 606, 1: 133}) Counter({2: 423, 0: 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965/965 [04:07<00:00,  3.90it/s]\n",
      "100%|██████████| 606/606 [02:28<00:00,  4.07it/s]\n",
      "100%|██████████| 133/133 [00:40<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 11 - flow\n",
      "Counter({0: 1489, 1: 83, 2: 43, 3: 25}) Counter({0: 410})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1489/1489 [04:29<00:00,  5.53it/s]\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.61it/s]\n",
      "100%|██████████| 83/83 [00:44<00:00,  1.85it/s]\n",
      "100%|██████████| 25/25 [00:12<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 12 - head\n",
      "Counter({2: 861, 0: 646, 1: 190, 3: 7}) Counter({1: 414, 3: 12})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 646/646 [02:13<00:00,  4.84it/s]\n",
      "100%|██████████| 861/861 [02:39<00:00,  5.41it/s]\n",
      "100%|██████████| 190/190 [00:49<00:00,  3.81it/s]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "complete_results = []\n",
    "for location, clust_n in zip(RUG.columns, clusters):\n",
    "    print(location)\n",
    "    data = get_data(location)\n",
    "\n",
    "    scaler, scaled_list_train, scaled_list_test = scale_data(data)\n",
    "    \n",
    "    pca_features = create_pca(scaled_list_train)\n",
    "\n",
    "    train_pca_features, test_pca_features = create_kmeans(pca_features, scaled_list_train, scaled_list_test, clust_n)\n",
    "    print(Counter(train_pca_features), Counter(test_pca_features))\n",
    "\n",
    "    # with open (fr\"C:\\Users\\mjnst\\Desktop\\results\\run_2_indices_{location}.txt\", 'wb') as f:\n",
    "    #     pickle.dump([train_pca_features, test_pca_features], f)\n",
    "\n",
    "    for cluster in [*Counter(train_pca_features)]:\n",
    "        cluster_train = scaled_list_train[np.where(train_pca_features == cluster)]\n",
    "        cluster_test = scaled_list_test[np.where(test_pca_features == cluster)]\n",
    "\n",
    "        reply = func(cluster_train, cluster_test, scaler)\n",
    "        # print([location, [cluster, [np.mean(reply[0]), np.mean(reply[1]), np.mean(reply[2]), np.mean(reply[3]), np.mean(reply[4]), np.mean(reply[5])]]])\n",
    "        complete_results.append([location, [cluster, [np.mean(reply[0]), np.mean(reply[1]), np.mean(reply[2]), np.mean(reply[3]), np.mean(reply[4]), np.mean(reply[5])]]])\n",
    "\n",
    "with open (r\"C:\\Users\\Martin\\Desktop\\thesis 2\\results\\run_4.txt\", 'wb') as f:\n",
    "    pickle.dump(complete_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
